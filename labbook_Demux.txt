----------------------------------
LAB NOTEBOOK - Demultiplexing Assignment
Author: Sophia Soriano
----------------------------------

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Part 1: Started 26JUL2022
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* Cloned new Demultiplex repo (created from Leslie's template) into Bi622 folder in Talapas
* Moved to /projects/bgmp/shared/2017_sequencing on Talapas to explore the four FASTQ data files
    ** 1294_S1_L008_R1_001.fastq.gz
    ** 1294_S1_L008_R2_001.fastq.gz
    ** 1294_S1_L008_R3_001.fastq.gz
    ** 1294_S1_L008_R4_001.fastq.gz
* Recording answers on Answers.md file
* Used the following command (with each file name above) to look at the first 10 lines of each file:
    ** $ zcat 1294_S1_L008_R1_001.fastq.gz | head
    ** The R1 and R4 files are read files (read 1 and read 2, respectively), and the R2 and R3 files are index files (index 1 and index 2, respectively)
    ** The Phred encoding appears to be Phred +33, as J is the highest encoded character.
* Used the following command (with each file name above) to find read lengths for each file
    ** $ zcat 1294_S1_L008_R1_001.fastq.gz | head -2 | tail -1 | wc
    ** Subtract 1 from results due to newline character
* Created QScore_Dist.py (Python 3.10) to generate a distribution of QScores per base position for a specified file name and read length.
* Installed numpy on Talapas and started interactive node with following command
    ** $ srun --account=bgmp --partition=bgmp --nodes=1 --ntasks-per-node=1 --time=2:00:00 --cpus-per-task=1 --pty bash
    ** Job 21722879 created and allocated to n278
* Created test file using the following command
    ** $ zcat /projects/bgmp/shared/2017_sequencing/1294_S1_L008_R1_001.fastq.gz | head -40 > /projects/bgmp/ssoriano/bioinfo/Bi622/Demultiplex/Assignment-the-first/test_P1.fq
    ** Test file ran successfully using QScore_Dist.py, bar graph looks reasonable
* Imported gzip to run zipped files in QScore_Dist.py
* Ran QScore_Dist.py with R1 FASTQ file, but output was specified to be in Leslie's directory...
    ** Added -o in argparse to specify output file name
* Created 4 slurm scripts (QScore.srun, QScoreR2.srun, QScoreR3.srun, QScoreR4.srun) to run each of the four input files
    ** Submitted each to Talapas: 21725656 (R1), 21725657 (R2), 21725658 (R3), 21725659 (R4)
* All four jobs completed successfully:
    ** 21725656 (R1 - read 1): time elapsed = 4:47:16 (h:m:s), Exit Status = 0 (see slurm-21725656.out)
    ** 21725657 (R2 - index 1): time elapsed = 30:35.17 (m:s), Exit Status = 0 (see slurm-21725657.out)
    ** 21725658 (R3 - index 2): time elapsed = 30:36.55 (m:s), Exit Status = 0 (see slurm-21725658.out)
    ** 21725659 (R4 - read 2): time elapsed = 4:47:12 (h:m:s), Exit Status = 0 (see slurm-21725659.out)
* Added all four histograms (QScore_read1.png, QScore_index1.png, QScore_index2.png, QScore_read2.png) to Answers.md file

27JUL2022

* Answered questions ii and iii in Answers.md
    ** For iii --> $ zcat 1294_S1_L008_R2_001.fastq.gz | grep -A1 "^@" | grep -v "^@" | grep -v "^--" | grep "N" | wc -l
    ** Repeated command for R3 file (Index 2)
    ** Index 1 file (R2) output = 3976613
    ** Index 2 file (R3) output = 3328051
    ** Calculated sum (7304664)

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Part 2: Started 26JUL2022
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* Created Demux_Pseudo.txt file to start drafting Demultiplexing algorithm strategy and pseudocode.

27JUL2022

* Used the following command for each data file (1294_S1_L008_R1_001.fastq.gz, R2, R3, R4) to make input test files:
    ** $ zcat 1294_S1_L008_R1_001.fastq.gz | head -1000000 | tail -12 > \
        /projects/bgmp/ssoriano/bioinfo/Bi622/Demultiplex/Assignment-the-first/test1_P2.fq
    ** This command isolates 3 records from a middle section of the file (head and tail too low quality)
    ** First record is a dual match (CTCTGGAT-ATCCAGAG), second record is an unknown (TCTTCGAC-GAAGAAAA), third
        record was a dual match (TACCGGAT-ATCCGGTA), but was changed to an index hopping case (AACAGCGA-ATCCGGTA).
    ** Known indexes found in indexes.txt in shared Talapas folder with the four FASTQ data files
    ** Test Input Files: test1_P2.fq, test2_P2.fq, test3_P2.fq, test4_P2.fq
* Created 6 test output files that are expected from the above four input files
    ** Test Output Files: test_output_DM_R1.fq, test_output_DM_R2.fq, test_output_IH_R1.fq, test_output_IH_R2.fq,
        test_output_U_R1.fq, test_output_U_R2.fq
    ** DM_R1: first record from R1 (read 1) file, added "index 1(from R2 file)-index2(from R3 file, rev. comp.)" to
        end of header line
    ** DM_R2: first record from R2 (read 2) file, added "index 1(from R2 file)-index2(from R3 file, rev. comp.)" to
        end of header line
    ** IH_R1: third record from R1 (read 1) file, added "index 1(from R2 file)-index2(from R3 file, rev. comp.)" to
        end of header line
    ** IH_R2: third record from R2 (read 1) file, added "index 1(from R2 file)-index2(from R3 file, rev. comp.)" to
        end of header line
    ** U_R1: second record from R1 (read 1) file, added "index 1(from R2 file)-index2(from R3 file, rev. comp.)" to
        end of header line
     ** U_R2: second record from R2 (read 1) file, added "index 1(from R2 file)-index2(from R3 file, rev. comp.)" to
        end of header line

28JUL2022

* Completed pseudocode file:
    ** Used while True loop to get one record (4 lines) at a time for each file - will need to decide if its easier to store each line in variables (16 variables) or use 4 lists w/ 4 items each.
    ** Changed from reading through five files (4 data files and indexes.txt) to creating a set of known indexes (column 5) from indexes.txt before opening the 4 data files and doing everything else. Set still allows me to check if the current index is contained in the set.
    ** Talked w/ Leslie about best (most time-effective) method to write to files - create a initial dictionary of lists containing 2 fh for each index1/IH/U
    ** See Demux_Pseudo.txt for full pseudocode w/ rational for each step
* Uploaded all Assignment-the-first files to github
    
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Part 3: Started 02AUG2022
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* Created Demux.py for Part 3 script
    ** Using type annotations for variables
    ** Will need to use argparse for input filnames (data and known indexes) before submittal
* Wrote reverse complement function and added to bioinfo.py (v0.4) - commented out in Demux.py
* Used "while True" loop and "with open" to open all four test input files simultaneously
    ** Fixed break condition (end of file) to only look at one file for empty string (avoid issues w/ multiple "or"s)
* Created "create_record()" function to take an (input) file handle and return a list containing the record lines
    ** Tested record creation for test input files - success!
* Wrote code to edit header lines of every iterated record
    ** Used separate "add_IndexPairHeader()" function, used to edit R1 and R4 record headers (0 index)
    ** Used rev_comp() function to get reverse complement of index2
        *** Should "N"s be at the same base loci (not reversed like other bases)?
        *** Not for this - use reverse complement.
* First "screening" conditional - if either index contains "N"
    ** Write R1 and R4 records to unknown R1 and R2 files
* Added code (before opening data files) to open known indexes file ("indexes.txt") and create set of known indexes
* Added open_outputs() function to open the 52 output files (48 dual matched (DM) - 24 each R1/R2, 2 index hopped (IH) - 1 each R1/R2, 2 unknown (U) - 1 each R1/R2)
    ** Used open_outputs() function in main code directly before opening 4 data (FASTQ) files
* Added close_outputs() function to close all 52 output file handles in fh_dict at end of main code.

08AUG2022

* Wrote write_file() function to write to output files (input: fh, record; output: None)
* Rewrote conditional to print "N" index records to unknown output files using write_file()
* Borrowed qscore mean calculator code from QScore_Dist.py
    ** Changed to make running sum per index read, not array of values
* Cutoff score for indexes set to 35 - indexes need to be very high quality in order to have confidence in demultiplexing ability
* Transferred qscore mean calculator to new function mean_qscore()
* Wrote conditional to write low quality index reads (<35) to unknown output files
* Wrote if/elif/elif conditional for true unknown index cases, index hopped cases, and dual-matched cases
* Set up argparse (still commented out for now) to read in 4 data files and the known indexes file
* Added counters (initialized before main code) to tally DM, IH, and U records (and sum all at end for total records count)
    ** Also included percentage calculations for each count/total count
* Added dictionaries to tally count of each index-pair type (as planned in Demux_Pseudo.txt)

09AUG2022

* Created Demux.srun slurm script to prepare for running final data files
    ** Used following SBATCH commands:
        *** #SBATCH --account=bgmp          ### SLURM account which will be charged for the job
        *** #SBATCH --partition=bgmp          ### partition to run things
        *** #SBATCH --job-name=Demux      ### Job Name
        *** #SBATCH --nodes=1              ### Node count required for the job
        *** #SBATCH --cpus-per-task=5       ### Number of cpus (cores) per task
        *** #SBATCH --mail-user=ssoriano@uoregon.edu ### Send email when done
        *** #SBATCH --mail-type=END
    ** Also added /usr/bin/time -v to front of command to capture runtime details
* Commented out dictionary to capture unknown index pairs/counts - not necessary for assignment and significantly elongates runtime.
* Added matplotlib plots to create distributions for number for occurances of each index-pair for DM and IH reads.
    ** Needed to remove second import matplotlib as plt2 statement and instead use plt.close() between creation of both plots
* Added back (uncommented) argparse in Demux.py
* Started to run Demux.srun but forgot to add back gzip/"rt" for large data files
* Restarted Demux.srun - job ID 21916045 node n278
    ** Completed run with Exit Status: 0
    ** Time Elapsed: 1:15:23 (h:m:s)
    ** Need to rerun to re-format distribution plots, but data seems to correlate w/ results of other classmates
* Created Demux.md file to write summary of output data - will need to add correct graphs later
* Removed IH graph code and added xticks rotation to DM_Dist.png code
* Moved output FASTQ files from job ID 21916045 to Demux_Output_Files
* Re-ran Demux.srun to generate improved DM_Dist.png graph
    ** Job ID: 21920408 node n278
    ** Exit Status: 0
    ** Time Elapsed: 1:20:28 (h:m:s)
* Axis labels tilted too far, will try again with rotation=30 instead of 45
    ** Job ID: 21928059 node n278
    ** Exit Status: 0
    ** Time Elapsed: 1:14:20 (h:m:s)
    ** Angle of x axis labels is sufficent to read one of the indexes, which is sufficient since these are dual-matched.
* Used pigz --fast "./Demux_Output_Files/*.fq" in for loop to zip files
    ** Ran in Demux_PigZ.srun slurm script (15 cpus)
    ** Job ID: 21931021
    ** Exit Status: 0
* Used following bash command to sort DM index section of output file by percentage (high -> low)
    ** $ head -33 slurm-21916045.out | tail -24 | sort -rnk 3
* Used following bash command to sort IH index section of output file by fraction (high -> low)
    ** Tried to sort by percentage, but "-rnk" options did not handle values with "e-05/e-06"
    ** $ head -587  slurm-21916045.out | tail -552 | sort -rnk 2
* Wrote up summaries for counts, DM index-pairs, and IH index-pairs in Demux.md file
    ** Included DM_Dist.png in DM_index-pairs section
* Uploaded all relevant code (NOT BIG FASTQ files) to GitHub